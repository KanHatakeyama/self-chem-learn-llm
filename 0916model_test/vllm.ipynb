{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:  1727109561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "pid = os.getpid()\n",
    "seed = int(pid)+int(datetime.now().timestamp())\n",
    "print(\"seed: \", seed)\n",
    "random.seed(seed)\n",
    "\n",
    "out_dir = \"data\"\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 17:31:41 config.py:904] Defaulting to use mp for distributed inference\n",
      "INFO 09-17 17:31:41 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-17 17:31:42 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-17 17:31:42 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m INFO 09-17 17:31:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:31:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-17 17:31:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m INFO 09-17 17:31:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-17 17:31:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m INFO 09-17 17:31:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:31:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-17 17:31:48 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-17 17:31:48 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:31:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-17 17:31:51 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fb4f5a243a0>, local_subscribe_port=52173, remote_subscribe_port=None)\n",
      "INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-17 17:31:51 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-17 17:31:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:31:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m INFO 09-17 17:31:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 17:31:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m INFO 09-17 17:31:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m INFO 09-17 17:31:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:31:53 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2554047754f34524b2a50c6ecc0d77ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m INFO 09-17 17:31:53 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 09-17 17:32:00 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m INFO 09-17 17:32:00 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:32:01 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m INFO 09-17 17:32:01 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:32:01 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m INFO 09-17 17:32:02 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m INFO 09-17 17:32:03 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m INFO 09-17 17:32:03 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "INFO 09-17 17:32:07 distributed_gpu_executor.py:57] # GPU blocks: 82648, # CPU blocks: 6553\n",
      "INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m INFO 09-17 17:32:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-17 17:32:23 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543391)\u001b[0;0m INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543387)\u001b[0;0m INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543390)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543385)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543388)\u001b[0;0m INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=543386)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=543389)\u001b[0;0m INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-17 17:32:23 model_runner.py:1430] Graph capturing finished in 14 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "#model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tensor_parallel_size = 8\n",
    "# トークナイザーとモデルの準備\n",
    "model = LLM(\n",
    "    model=model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=2000,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.7\n",
    "top_k=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_gen(model,prompt_list,temperature=0.7,top_k=50):\n",
    "    outputs = model.generate(\n",
    "        prompt_list,\n",
    "        sampling_params=SamplingParams(\n",
    "            temperature=temperature,\n",
    "            max_tokens=1024,\n",
    "            #repetition_penalty=1.2,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    )\n",
    "    return [i.outputs[0].text.strip() for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer=transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CompName': 'unknown',\n",
       " 'SMILES': 'C1=CC(=CC(=C1)CN)CN',\n",
       " 'Property': 'Boiling temperature',\n",
       " 'Value': 247.2222222,\n",
       " 'unit': '[oC]',\n",
       " 'Source': 'Wikipedia/Wikidata'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds=load_dataset(\"kanhatakeyama/material-properties\",split=\"train\")\n",
    "ds=ds.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_problem(record):\n",
    "    comp_name=record[\"CompName\"]\n",
    "    smiles=record[\"SMILES\"]\n",
    "    unit=record[\"unit\"]\n",
    "    property_name=record[\"Property\"]\n",
    "    q=f\"\"\"Predict the {property_name} {unit} of the following compound. \n",
    "    #Restriction: Output must contain \"#Reason\" section which explains the step-by-step quantitative reasoning behind the prediction. Output must also contain \"#Predicted\" section which contains only the predicted value (number only).\n",
    "    #Name: {comp_name}\n",
    "    #SMILES: {smiles}\"\"\"\n",
    "    actual_value=record[\"Value\"]\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "\n",
    "\n",
    "    prompt=tokenizer.apply_chat_template(chat,tokenize=False,)\n",
    "    assist_prefix=\"assistant\\n\\n#Reason\\n\"\n",
    "    prompt+=assist_prefix\n",
    "\n",
    "    return prompt,actual_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:26<00:00,  3.81it/s, est. speed input: 452.12 toks/s, output: 1266.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:21<00:00,  4.74it/s, est. speed input: 569.54 toks/s, output: 1508.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.30it/s, est. speed input: 517.89 toks/s, output: 1401.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.19it/s, est. speed input: 507.46 toks/s, output: 1419.79 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:24<00:00,  4.09it/s, est. speed input: 496.74 toks/s, output: 1381.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.51it/s, est. speed input: 429.26 toks/s, output: 1120.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.18it/s, est. speed input: 504.61 toks/s, output: 1320.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:25<00:00,  3.90it/s, est. speed input: 465.88 toks/s, output: 1275.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:18<00:00,  5.27it/s, est. speed input: 635.70 toks/s, output: 1669.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:19<00:00,  5.05it/s, est. speed input: 608.00 toks/s, output: 1553.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:22<00:00,  4.43it/s, est. speed input: 542.13 toks/s, output: 1431.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:20<00:00,  4.83it/s, est. speed input: 593.42 toks/s, output: 1510.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.46it/s, est. speed input: 418.41 toks/s, output: 1127.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:26<00:00,  3.71it/s, est. speed input: 446.85 toks/s, output: 1203.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:24<00:00,  4.05it/s, est. speed input: 484.54 toks/s, output: 1281.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:22<00:00,  4.48it/s, est. speed input: 547.30 toks/s, output: 1417.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:22<00:00,  4.42it/s, est. speed input: 532.07 toks/s, output: 1380.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:29<00:00,  3.44it/s, est. speed input: 414.33 toks/s, output: 1187.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s, est. speed input: 591.26 toks/s, output: 1573.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:20<00:00,  4.90it/s, est. speed input: 587.01 toks/s, output: 1387.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s, est. speed input: 418.14 toks/s, output: 1120.47 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.23it/s, est. speed input: 512.34 toks/s, output: 1315.56 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s, est. speed input: 486.14 toks/s, output: 1286.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s, est. speed input: 585.58 toks/s, output: 1473.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.28it/s, est. speed input: 520.74 toks/s, output: 1322.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.50it/s, est. speed input: 429.68 toks/s, output: 1135.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.28it/s, est. speed input: 534.17 toks/s, output: 1444.24 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:24<00:00,  4.02it/s, est. speed input: 479.10 toks/s, output: 1340.49 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:21<00:00,  4.74it/s, est. speed input: 575.29 toks/s, output: 1474.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.56it/s, est. speed input: 431.37 toks/s, output: 1141.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:20<00:00,  4.83it/s, est. speed input: 585.96 toks/s, output: 1432.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:19<00:00,  5.05it/s, est. speed input: 606.78 toks/s, output: 1634.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s, est. speed input: 421.31 toks/s, output: 1174.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:28<00:00,  3.46it/s, est. speed input: 424.47 toks/s, output: 1157.09 toks/s]\n",
      " 34%|███▍      | 34/100 [13:58<27:06, 24.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     prompt_list\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#interference\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m predicted_text_list\u001b[38;5;241m=\u001b[39m\u001b[43mllm_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#parse\u001b[39;00m\n\u001b[1;32m     25\u001b[0m good_records\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mllm_gen\u001b[0;34m(model, prompt_list, temperature, top_k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm_gen\u001b[39m(model,prompt_list,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSamplingParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#repetition_penalty=1.2,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [i\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/utils.py:1036\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1033\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         )\n\u001b[0;32m-> 1036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/entrypoints/llm.py:348\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request)\u001b[0m\n\u001b[1;32m    339\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    342\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    343\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[1;32m    344\u001b[0m     lora_request\u001b[38;5;241m=\u001b[39mlora_request,\n\u001b[1;32m    345\u001b[0m     prompt_adapter_request\u001b[38;5;241m=\u001b[39mprompt_adapter_request,\n\u001b[1;32m    346\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request)\n\u001b[0;32m--> 348\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/entrypoints/llm.py:715\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    713\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 715\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/engine/llm_engine.py:1223\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1220\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1221\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:78\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_worker_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_worker_execution_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m         async_run_tensor_parallel_workers_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m driver_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver_execute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m driver_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:162\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._driver_execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_driver_execute_model\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: Optional[ExecuteModelRequest]\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[SamplerOutput]]:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run execute_model in the driver worker.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Passing None will cause the driver to stop the model execution\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    loop running in each of the remote workers.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/worker/worker_base.py:327\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    324\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 327\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/worker/model_runner_base.py:112\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    114\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/worker/model_runner.py:1589\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     model_input\u001b[38;5;241m.\u001b[39masync_callback()\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m-> 1589\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1596\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:466\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    463\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    464\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 466\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:278\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    275\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m maybe_deferred_sample_results, maybe_sampled_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_gpu_probs_tensor:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:968\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    949\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    950\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleReturnType:\n\u001b[1;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:855\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    843\u001b[0m maybe_deferred_args \u001b[38;5;241m=\u001b[39m SampleResultArgsType(\n\u001b[1;32m    844\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39msampling_metadata,\n\u001b[1;32m    845\u001b[0m     sample_metadata\u001b[38;5;241m=\u001b[39msample_metadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     beam_search_logprobs\u001b[38;5;241m=\u001b[39mbeam_search_logprobs,\n\u001b[1;32m    849\u001b[0m     sample_results_dict\u001b[38;5;241m=\u001b[39msample_results_dict)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata\u001b[38;5;241m.\u001b[39mskip_sampler_cpu_output:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_pythonized_sample_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaybe_deferred_args\u001b[49m\u001b[43m)\u001b[49m, sampled_token_ids_tensor\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    861\u001b[0m         maybe_deferred_args,\n\u001b[1;32m    862\u001b[0m         sampled_token_ids_tensor,\n\u001b[1;32m    863\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:720\u001b[0m, in \u001b[0;36mget_pythonized_sample_results\u001b[0;34m(sample_result_args)\u001b[0m\n\u001b[1;32m    718\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 720\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    723\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups,\n\u001b[1;32m    724\u001b[0m                                          beam_search_logprobs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:519\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, random_samples)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    521\u001b[0m results: SampleResultType \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "n_records=100\n",
    "for _ in tqdm(range(100)):\n",
    "    problems=[]\n",
    "    prompt_list=[\n",
    "    ]\n",
    "    for i in range(n_records):\n",
    "        record=random.choice(ds)\n",
    "        prompt,actual_value=gen_problem(record)\n",
    "        problems.append(\n",
    "            {\n",
    "            \"record\":record,\n",
    "            \"prompt\":prompt,\n",
    "            \"actual_value\":actual_value   \n",
    "            }\n",
    "\n",
    "        )\n",
    "        prompt_list.append(prompt)\n",
    "\n",
    "    #interference\n",
    "    predicted_text_list=llm_gen(model,prompt_list)\n",
    "\n",
    "\n",
    "    #parse\n",
    "    good_records=[]\n",
    "    for i in range(n_records):\n",
    "        problems[i][\"CompoundName\"]=problems[i][\"record\"][\"CompName\"]\n",
    "        problems[i][\"SMILES\"]=problems[i][\"record\"][\"SMILES\"]\n",
    "        problems[i][\"Property\"]=problems[i][\"record\"][\"Property\"]\n",
    "        problems[i][\"Unit\"]=problems[i][\"record\"][\"unit\"]\n",
    "        problems[i][\"predicted\"]=predicted_text_list[i].split(\"#Predicted\\n\")[-1]\n",
    "        problems[i][\"predicted_text\"]=predicted_text_list[i]\n",
    "        #誤差率\n",
    "        problems[i][\"actual_value\"]=float(problems[i][\"actual_value\"])\n",
    "        try:\n",
    "            error=abs((problems[i][\"actual_value\"]-float(problems[i][\"predicted\"]))/problems[i][\"actual_value\"])\n",
    "            problems[i][\"error_rate\"]=error\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        with open(f\"data/{current_time_no_symbols}_llm_gen.jsonl\",\"a\", encoding='utf-8') as f:\n",
    "            f.write(json.dumps(problems[i],ensure_ascii=False)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df=pd.DataFrame(problems)\n",
    "#df=df.drop(columns=[\"record\",\"prompt\"])\n",
    "#df.to_csv(\"predicted.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_value</th>\n",
       "      <th>CompoundName</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Property</th>\n",
       "      <th>Unit</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted_text</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>Dicobalt octacarbonyl</td>\n",
       "      <td>O=C=[Co]1(=C=O)(=C=O)C(=O)[Co](=C=O)(=C=O)(=C=...</td>\n",
       "      <td>Boiling temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>152</td>\n",
       "      <td>To predict the boiling temperature of dicobalt...</td>\n",
       "      <td>1.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>Benzoyl chloride</td>\n",
       "      <td>ClC(=O)c1ccccc1</td>\n",
       "      <td>Melting temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>39.5</td>\n",
       "      <td>To predict the melting temperature of Benzoyl ...</td>\n",
       "      <td>40.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>CC1=CC=C(C=C1)N</td>\n",
       "      <td>Ionization Energy</td>\n",
       "      <td>[eV]</td>\n",
       "      <td>6.5</td>\n",
       "      <td>To predict the ionization energy of the given ...</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.100000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>CC(=CC1=CC=CC=C1)[N+](=O)[O-]</td>\n",
       "      <td>Density</td>\n",
       "      <td>[g/cm3]</td>\n",
       "      <td>1.34</td>\n",
       "      <td>To predict the density of the given compound, ...</td>\n",
       "      <td>0.218182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.900000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C(=O)(C(=O)O)O</td>\n",
       "      <td>Density</td>\n",
       "      <td>[g/cm3]</td>\n",
       "      <td>1.05</td>\n",
       "      <td>To predict the density of the given compound, ...</td>\n",
       "      <td>0.447368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>166.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C1=CC=C(C=C1)[Ga](C2=CC=CC=C2)C3=CC=CC=C3</td>\n",
       "      <td>Melting temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>0</td>\n",
       "      <td>To predict the melting temperature of the give...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>CCC(C)OC(=O)C</td>\n",
       "      <td>Vapor pressure</td>\n",
       "      <td>[kPa]</td>\n",
       "      <td>0.018</td>\n",
       "      <td>To predict the vapor pressure of the given com...</td>\n",
       "      <td>0.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>243.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C(C(F)(F)F)(Cl)Br</td>\n",
       "      <td>Vapor pressure</td>\n",
       "      <td>[kPa]</td>\n",
       "      <td>0.035</td>\n",
       "      <td>To predict the vapor pressure of the compound ...</td>\n",
       "      <td>0.999856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.111111</td>\n",
       "      <td>unknown</td>\n",
       "      <td>CC(=CC(=O)OC)OP(=O)(OC)OC</td>\n",
       "      <td>Melting temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>128.23</td>\n",
       "      <td>To predict the melting temperature of the give...</td>\n",
       "      <td>5.074053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.300000</td>\n",
       "      <td>Ethylene</td>\n",
       "      <td>C=C</td>\n",
       "      <td>Absolute molar magnetic susceptibility</td>\n",
       "      <td>[10^-6 cm3/mol]</td>\n",
       "      <td>0</td>\n",
       "      <td>To predict the absolute molar magnetic suscept...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.391450</td>\n",
       "      <td>unknown</td>\n",
       "      <td>CC(C)CC(C)(C)C</td>\n",
       "      <td>Refractive index</td>\n",
       "      <td>[-]</td>\n",
       "      <td>1.43</td>\n",
       "      <td>To predict the refractive index of the given c...</td>\n",
       "      <td>0.027705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C(CBr)SCCBr</td>\n",
       "      <td>Decomposition temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>226.85</td>\n",
       "      <td>To predict the decomposition temperature of th...</td>\n",
       "      <td>0.054792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C1=CC=NC=C1</td>\n",
       "      <td>Vapor pressure</td>\n",
       "      <td>[kPa]</td>\n",
       "      <td>25.3</td>\n",
       "      <td>To predict the vapor pressure of the given com...</td>\n",
       "      <td>0.581250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>287.200000</td>\n",
       "      <td>Phenylpiperazine</td>\n",
       "      <td>c1cc(ccc1)N2CCNCC2</td>\n",
       "      <td>Boiling temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>262</td>\n",
       "      <td>To predict the boiling temperature of Phenylpi...</td>\n",
       "      <td>0.087744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3.888889</td>\n",
       "      <td>unknown</td>\n",
       "      <td>CC1CCCCC1</td>\n",
       "      <td>Flash temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>242.79</td>\n",
       "      <td>To predict the flash temperature of the given ...</td>\n",
       "      <td>63.431714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>158.000000</td>\n",
       "      <td>Sarin</td>\n",
       "      <td>FP(=O)(OC(C)C)C</td>\n",
       "      <td>Boiling temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>67</td>\n",
       "      <td>To predict the boiling temperature of Sarin, w...</td>\n",
       "      <td>0.575949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C(C(Cl)Cl)(Cl)Cl</td>\n",
       "      <td>Vapor pressure</td>\n",
       "      <td>[kPa]</td>\n",
       "      <td>1</td>\n",
       "      <td>To predict the vapor pressure of the given com...</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.500000</td>\n",
       "      <td>Aluminium chloride</td>\n",
       "      <td>Cl[Al](Cl)Cl</td>\n",
       "      <td>Absolute standard enthalpy of formation</td>\n",
       "      <td>[kJ/mol]</td>\n",
       "      <td>0</td>\n",
       "      <td>The absolute standard enthalpy of formation of...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-112.222222</td>\n",
       "      <td>unknown</td>\n",
       "      <td>P(Cl)(Cl)Cl</td>\n",
       "      <td>Melting temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>191.5</td>\n",
       "      <td>The compound with the SMILES notation \"P(Cl)(C...</td>\n",
       "      <td>2.706436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>220.000000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>C1=CC=C(C=C1)OP(=O)(OC2=CC=CC=C2)OC3=CC=CC=C3</td>\n",
       "      <td>Flash temperature</td>\n",
       "      <td>[oC]</td>\n",
       "      <td>162.5</td>\n",
       "      <td>To predict the Flash temperature of the given ...</td>\n",
       "      <td>0.261364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual_value           CompoundName  \\\n",
       "0      52.000000  Dicobalt octacarbonyl   \n",
       "1      -1.000000       Benzoyl chloride   \n",
       "2       7.500000                unknown   \n",
       "3       1.100000                unknown   \n",
       "4       1.900000                unknown   \n",
       "5     166.000000                unknown   \n",
       "6      10.000000                unknown   \n",
       "7     243.000000                unknown   \n",
       "8      21.111111                unknown   \n",
       "9      15.300000               Ethylene   \n",
       "10      1.391450                unknown   \n",
       "11    240.000000                unknown   \n",
       "12     16.000000                unknown   \n",
       "13    287.200000       Phenylpiperazine   \n",
       "14     -3.888889                unknown   \n",
       "15    158.000000                  Sarin   \n",
       "16      5.000000                unknown   \n",
       "17      5.500000     Aluminium chloride   \n",
       "18   -112.222222                unknown   \n",
       "19    220.000000                unknown   \n",
       "\n",
       "                                               SMILES  \\\n",
       "0   O=C=[Co]1(=C=O)(=C=O)C(=O)[Co](=C=O)(=C=O)(=C=...   \n",
       "1                                     ClC(=O)c1ccccc1   \n",
       "2                                     CC1=CC=C(C=C1)N   \n",
       "3                       CC(=CC1=CC=CC=C1)[N+](=O)[O-]   \n",
       "4                                      C(=O)(C(=O)O)O   \n",
       "5           C1=CC=C(C=C1)[Ga](C2=CC=CC=C2)C3=CC=CC=C3   \n",
       "6                                       CCC(C)OC(=O)C   \n",
       "7                                   C(C(F)(F)F)(Cl)Br   \n",
       "8                           CC(=CC(=O)OC)OP(=O)(OC)OC   \n",
       "9                                                 C=C   \n",
       "10                                     CC(C)CC(C)(C)C   \n",
       "11                                        C(CBr)SCCBr   \n",
       "12                                        C1=CC=NC=C1   \n",
       "13                                 c1cc(ccc1)N2CCNCC2   \n",
       "14                                          CC1CCCCC1   \n",
       "15                                    FP(=O)(OC(C)C)C   \n",
       "16                                   C(C(Cl)Cl)(Cl)Cl   \n",
       "17                                       Cl[Al](Cl)Cl   \n",
       "18                                        P(Cl)(Cl)Cl   \n",
       "19      C1=CC=C(C=C1)OP(=O)(OC2=CC=CC=C2)OC3=CC=CC=C3   \n",
       "\n",
       "                                   Property             Unit predicted  \\\n",
       "0                       Boiling temperature             [oC]       152   \n",
       "1                       Melting temperature             [oC]      39.5   \n",
       "2                         Ionization Energy             [eV]       6.5   \n",
       "3                                   Density          [g/cm3]      1.34   \n",
       "4                                   Density          [g/cm3]      1.05   \n",
       "5                       Melting temperature             [oC]         0   \n",
       "6                            Vapor pressure            [kPa]     0.018   \n",
       "7                            Vapor pressure            [kPa]     0.035   \n",
       "8                       Melting temperature             [oC]    128.23   \n",
       "9    Absolute molar magnetic susceptibility  [10^-6 cm3/mol]         0   \n",
       "10                         Refractive index              [-]      1.43   \n",
       "11                Decomposition temperature             [oC]    226.85   \n",
       "12                           Vapor pressure            [kPa]      25.3   \n",
       "13                      Boiling temperature             [oC]       262   \n",
       "14                        Flash temperature             [oC]    242.79   \n",
       "15                      Boiling temperature             [oC]        67   \n",
       "16                           Vapor pressure            [kPa]         1   \n",
       "17  Absolute standard enthalpy of formation         [kJ/mol]         0   \n",
       "18                      Melting temperature             [oC]     191.5   \n",
       "19                        Flash temperature             [oC]     162.5   \n",
       "\n",
       "                                       predicted_text  error_rate  \n",
       "0   To predict the boiling temperature of dicobalt...    1.923077  \n",
       "1   To predict the melting temperature of Benzoyl ...   40.500000  \n",
       "2   To predict the ionization energy of the given ...    0.133333  \n",
       "3   To predict the density of the given compound, ...    0.218182  \n",
       "4   To predict the density of the given compound, ...    0.447368  \n",
       "5   To predict the melting temperature of the give...    1.000000  \n",
       "6   To predict the vapor pressure of the given com...    0.998200  \n",
       "7   To predict the vapor pressure of the compound ...    0.999856  \n",
       "8   To predict the melting temperature of the give...    5.074053  \n",
       "9   To predict the absolute molar magnetic suscept...    1.000000  \n",
       "10  To predict the refractive index of the given c...    0.027705  \n",
       "11  To predict the decomposition temperature of th...    0.054792  \n",
       "12  To predict the vapor pressure of the given com...    0.581250  \n",
       "13  To predict the boiling temperature of Phenylpi...    0.087744  \n",
       "14  To predict the flash temperature of the given ...   63.431714  \n",
       "15  To predict the boiling temperature of Sarin, w...    0.575949  \n",
       "16  To predict the vapor pressure of the given com...    0.800000  \n",
       "17  The absolute standard enthalpy of formation of...    1.000000  \n",
       "18  The compound with the SMILES notation \"P(Cl)(C...    2.706436  \n",
       "19  To predict the Flash temperature of the given ...    0.261364  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
