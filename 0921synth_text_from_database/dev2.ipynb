{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "seed:  1727714276\n"
     ]
    }
   ],
   "source": [
    "#auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from src.llm.PredictionUtils import init_model_and_tokenizer,llm_gen\n",
    "from src.prop.utils import gen_reason,gen_masked_prediction_problem_prompts,parse_prediction_with_check\n",
    "\n",
    "pid = os.getpid()\n",
    "seed = int(pid)+int(datetime.now().timestamp())\n",
    "print(\"seed: \", seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "out_dir = \"data\"\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 16:55:33 config.py:904] Defaulting to use mp for distributed inference\n",
      "INFO 09-21 16:55:33 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-21 16:55:34 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-21 16:55:34 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:55:34 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:55:34 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:55:34 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:55:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m INFO 09-21 16:55:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m INFO 09-21 16:55:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m INFO 09-21 16:55:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-21 16:55:40 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-21 16:55:40 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-21 16:55:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-21 16:55:44 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fd0fff94cd0>, local_subscribe_port=50495, remote_subscribe_port=None)\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-21 16:55:44 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:55:44 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:55:45 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m INFO 09-21 16:55:45 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:55:45 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:55:45 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 09-21 16:55:45 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 09-21 16:55:45 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50554028d383404b89b9fffa8f0d8d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 16:55:53 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m INFO 09-21 16:55:53 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:55:54 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m INFO 09-21 16:55:54 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:55:54 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:55:55 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m INFO 09-21 16:55:56 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:55:56 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "INFO 09-21 16:55:59 distributed_gpu_executor.py:57] # GPU blocks: 82648, # CPU blocks: 6553\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:56:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-21 16:56:17 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809058)\u001b[0;0m INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 15 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809057)\u001b[0;0m INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 15 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809055)\u001b[0;0m INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 15 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809056)\u001b[0;0m INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=809060)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809059)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=809061)\u001b[0;0m INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 15 secs.\n",
      "INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 15 secs.\n",
      "INFO 09-21 16:56:17 model_runner.py:1430] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "model,tokenizer=init_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds=load_dataset(\"kanhatakeyama/material-properties\",split=\"train\")\n",
    "ds=ds.shuffle(seed=1)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CompName': 'unknown',\n",
       " 'SMILES': 'CC1=CC=C(C=C1)O',\n",
       " 'Property': 'Vapor pressure',\n",
       " 'Value': 0.11,\n",
       " 'unit': '[kPa]',\n",
       " 'Source': 'Wikipedia/Wikidata'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds=ds.select(range(7000))\n",
    "test_ds=ds.select(range(7000,7200))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:09<00:00,  3.23s/it, est. speed input: 82.41 toks/s, output: 102.94 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "target_ds=train_ds\n",
    "\n",
    "def record_to_list_text(record):\n",
    "    record=copy.deepcopy(record)\n",
    "    record.pop(\"Source\")\n",
    "    text=json.dumps(record)\n",
    "    return text\n",
    "\n",
    "\n",
    "style_list=[\n",
    "    \"textbook-style\",\n",
    "    \"conversation-style\",\n",
    "    \"classroom-style\",\n",
    "    \"Q&A-style\",\n",
    "    \"interview-style\",\n",
    "    \"journal-style\",\n",
    "]\n",
    "prompt_template=\"\"\"Prepare {} text according to the following data.\n",
    "- Never include any other number which is not given from the data.\n",
    "- Generate only the text, not e.g., system outputs.\n",
    "- Add some discussion or explanation if necessary.\n",
    "\"\"\"\n",
    "\n",
    "def gen_prompt(tokenizer,target_ds,style_list):\n",
    "    style=random.choice(style_list)\n",
    "    q=prompt_template.format(style)\n",
    "\n",
    "    for i in range(random.randint(1,4)):\n",
    "        record=target_ds[random.randint(0,len(target_ds)-1)]\n",
    "        q+=record_to_list_text(record)+\"\\n\"\n",
    "    \n",
    "    q=q.strip()\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False,)\n",
    "    assist_prefix = \"assistant\\n\\n\"\n",
    "    prompt += assist_prefix\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    prompt_list=[(gen_prompt(tokenizer,target_ds,style_list)) for i in range(batch_size)]\n",
    "    predicted_text_list=llm_gen(model,prompt_list)\n",
    "\n",
    "    for i in range(len(predicted_text_list)):\n",
    "        with open(f\"data/{current_time_no_symbols}_llm_gen.jsonl\",\"a\", encoding='utf-8') as f:\n",
    "            f.write(json.dumps({\"text\":predicted_text_list[i]},ensure_ascii=False)+\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
