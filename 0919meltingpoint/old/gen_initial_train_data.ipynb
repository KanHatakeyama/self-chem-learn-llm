{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:  1727393140\n"
     ]
    }
   ],
   "source": [
    "#auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from src.llm.PredictionUtils import init_model_and_tokenizer,llm_gen\n",
    "from src.prop.utils import gen_reason,gen_masked_prediction_problem_prompts,parse_prediction_with_check\n",
    "\n",
    "pid = os.getpid()\n",
    "seed = int(pid)+int(datetime.now().timestamp())\n",
    "print(\"seed: \", seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "out_dir = \"data\"\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 15:17:47 config.py:904] Defaulting to use mp for distributed inference\n",
      "INFO 09-19 15:17:47 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-19 15:17:48 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-19 15:17:48 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:17:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:17:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m INFO 09-19 15:17:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:17:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m INFO 09-19 15:17:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:17:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:17:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:17:54 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:17:54 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:17:57 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:17:58 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f979c3bc310>, local_subscribe_port=45079, remote_subscribe_port=None)\n",
      "INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:17:58 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:17:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:17:59 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98d426d831643f6a382d6efb09d6f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:18:07 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m INFO 09-19 15:18:07 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:18:07 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "INFO 09-19 15:18:08 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m INFO 09-19 15:18:08 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:18:08 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:18:09 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:18:09 model_runner.py:1008] Loading model weights took 16.4605 GB\n",
      "INFO 09-19 15:18:13 distributed_gpu_executor.py:57] # GPU blocks: 82648, # CPU blocks: 6553\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:18:15 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:18:15 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m INFO 09-19 15:18:16 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:18:30 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666600)\u001b[0;0m INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666604)\u001b[0;0m INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666601)\u001b[0;0m INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666602)\u001b[0;0m INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 15 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=666603)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=666605)\u001b[0;0m INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n",
      "INFO 09-19 15:18:30 model_runner.py:1430] Graph capturing finished in 14 secs.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "model,tokenizer=init_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881e863784164edb920736512be65c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05f6f07d924ef5b3af14ab83491cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bradley-00000-of-00001.parquet:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc87dfd05b174487af64efc1fb84ced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24ee45b15fb429f85fd879204ab1ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Bradley split:   0%|          | 0/24889 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24889"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds=load_dataset(\"kanhatakeyama/material-properties\",split=\"Bradley\")\n",
    "ds=ds.shuffle(seed=1)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CompName': '(2Z)-2-(1,3-Benzodioxol-5-ylmethylene)-6,7-dihydro-5H-[1,3]thiazolo[3,2-a]pyrimidin-3(2H)-one',\n",
       " 'SMILES': 'O=C1C(SC2=NCCCN12)=Cc4ccc3OCOc3c4',\n",
       " 'Property': 'Melting temperature',\n",
       " 'Value': 151.35,\n",
       " 'unit': '[oC]',\n",
       " 'Source': 'BradleyMeltingPointDataset'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds=ds.select(range(24000))\n",
    "test_ds=ds.select(range(24000,24800))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [01:00<00:00,  8.24it/s, est. speed input: 1275.55 toks/s, output: 2065.62 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.33it/s, est. speed input: 8124.80 toks/s, output: 83.56 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:58<00:00,  8.50it/s, est. speed input: 1316.64 toks/s, output: 2116.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.47it/s, est. speed input: 8110.03 toks/s, output: 87.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:56<00:00,  8.87it/s, est. speed input: 1376.13 toks/s, output: 2227.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.38it/s, est. speed input: 8139.79 toks/s, output: 84.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:52<00:00,  9.49it/s, est. speed input: 1462.62 toks/s, output: 2340.87 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.82it/s, est. speed input: 7878.89 toks/s, output: 86.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:59<00:00,  8.33it/s, est. speed input: 1294.32 toks/s, output: 2084.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.44it/s, est. speed input: 8135.59 toks/s, output: 85.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  8.97it/s, est. speed input: 1385.01 toks/s, output: 2213.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.61it/s, est. speed input: 8118.77 toks/s, output: 83.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:51<00:00,  9.63it/s, est. speed input: 1480.81 toks/s, output: 2378.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.71it/s, est. speed input: 8139.07 toks/s, output: 83.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:00<00:00,  8.31it/s, est. speed input: 1289.37 toks/s, output: 2035.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.63it/s, est. speed input: 7722.87 toks/s, output: 86.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  9.08it/s, est. speed input: 1398.03 toks/s, output: 2244.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.65it/s, est. speed input: 8120.11 toks/s, output: 85.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:02<00:00,  8.03it/s, est. speed input: 1246.42 toks/s, output: 2019.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.94it/s, est. speed input: 8028.65 toks/s, output: 81.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:49<00:00, 10.19it/s, est. speed input: 1582.56 toks/s, output: 2510.14 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:21<00:00, 22.97it/s, est. speed input: 6996.81 toks/s, output: 80.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:01<00:00,  8.12it/s, est. speed input: 1260.30 toks/s, output: 2058.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.02it/s, est. speed input: 8121.05 toks/s, output: 84.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:02<00:00,  8.04it/s, est. speed input: 1244.69 toks/s, output: 2048.62 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.53it/s, est. speed input: 7968.56 toks/s, output: 85.43 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:59<00:00,  8.39it/s, est. speed input: 1291.02 toks/s, output: 2089.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.44it/s, est. speed input: 8125.95 toks/s, output: 85.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:01<00:00,  8.10it/s, est. speed input: 1252.90 toks/s, output: 2009.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.17it/s, est. speed input: 8020.98 toks/s, output: 84.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:56<00:00,  8.81it/s, est. speed input: 1362.69 toks/s, output: 2182.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.49it/s, est. speed input: 8112.82 toks/s, output: 85.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  8.99it/s, est. speed input: 1384.93 toks/s, output: 2224.50 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.73it/s, est. speed input: 8121.79 toks/s, output: 84.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:57<00:00,  8.64it/s, est. speed input: 1337.49 toks/s, output: 2148.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.49it/s, est. speed input: 8107.08 toks/s, output: 84.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:59<00:00,  8.40it/s, est. speed input: 1297.84 toks/s, output: 2109.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.41it/s, est. speed input: 8138.77 toks/s, output: 83.60 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:01<00:00,  8.14it/s, est. speed input: 1264.78 toks/s, output: 2014.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.68it/s, est. speed input: 8143.93 toks/s, output: 85.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:57<00:00,  8.75it/s, est. speed input: 1347.84 toks/s, output: 2235.40 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.72it/s, est. speed input: 8067.32 toks/s, output: 84.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:53<00:00,  9.34it/s, est. speed input: 1450.79 toks/s, output: 2293.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.86it/s, est. speed input: 8117.55 toks/s, output: 87.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:52<00:00,  9.54it/s, est. speed input: 1483.52 toks/s, output: 2338.47 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.88it/s, est. speed input: 7825.00 toks/s, output: 84.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:52<00:00,  9.47it/s, est. speed input: 1470.43 toks/s, output: 2402.56 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.97it/s, est. speed input: 8071.55 toks/s, output: 81.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:00<00:00,  8.26it/s, est. speed input: 1277.62 toks/s, output: 2082.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.36it/s, est. speed input: 7850.22 toks/s, output: 83.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:59<00:00,  8.35it/s, est. speed input: 1288.33 toks/s, output: 2040.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.22it/s, est. speed input: 7933.10 toks/s, output: 84.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:00<00:00,  8.26it/s, est. speed input: 1279.16 toks/s, output: 2028.55 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.63it/s, est. speed input: 8055.14 toks/s, output: 85.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:53<00:00,  9.41it/s, est. speed input: 1463.14 toks/s, output: 2371.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.33it/s, est. speed input: 7815.60 toks/s, output: 86.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:54<00:00,  9.24it/s, est. speed input: 1432.40 toks/s, output: 2309.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.35it/s, est. speed input: 8119.60 toks/s, output: 86.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  9.04it/s, est. speed input: 1400.29 toks/s, output: 2277.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.27it/s, est. speed input: 8131.27 toks/s, output: 85.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:01<00:00,  8.18it/s, est. speed input: 1258.78 toks/s, output: 2039.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.87it/s, est. speed input: 7941.14 toks/s, output: 88.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:51<00:00,  9.67it/s, est. speed input: 1497.13 toks/s, output: 2404.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.58it/s, est. speed input: 8141.00 toks/s, output: 84.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:56<00:00,  8.81it/s, est. speed input: 1364.46 toks/s, output: 2219.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.12it/s, est. speed input: 8068.88 toks/s, output: 85.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:56<00:00,  8.82it/s, est. speed input: 1362.31 toks/s, output: 2255.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.31it/s, est. speed input: 7928.71 toks/s, output: 82.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  8.95it/s, est. speed input: 1396.63 toks/s, output: 2267.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.04it/s, est. speed input: 8078.87 toks/s, output: 82.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  8.98it/s, est. speed input: 1387.33 toks/s, output: 2233.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.56it/s, est. speed input: 8129.17 toks/s, output: 85.11 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:55<00:00,  8.97it/s, est. speed input: 1386.56 toks/s, output: 2182.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.91it/s, est. speed input: 8119.03 toks/s, output: 85.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:57<00:00,  8.77it/s, est. speed input: 1344.36 toks/s, output: 2177.55 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.38it/s, est. speed input: 8051.02 toks/s, output: 87.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:02<00:00,  7.97it/s, est. speed input: 1233.67 toks/s, output: 2013.75 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:18<00:00, 26.42it/s, est. speed input: 8174.05 toks/s, output: 82.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:01<00:00,  8.18it/s, est. speed input: 1272.31 toks/s, output: 2091.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.10it/s, est. speed input: 7860.27 toks/s, output: 84.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:53<00:00,  9.37it/s, est. speed input: 1444.35 toks/s, output: 2400.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.63it/s, est. speed input: 8048.76 toks/s, output: 84.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [01:01<00:00,  8.13it/s, est. speed input: 1250.28 toks/s, output: 2032.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.12it/s, est. speed input: 8045.74 toks/s, output: 85.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:53<00:00,  9.36it/s, est. speed input: 1449.09 toks/s, output: 2290.84 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.24it/s, est. speed input: 7920.98 toks/s, output: 85.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:57<00:00,  8.67it/s, est. speed input: 1340.97 toks/s, output: 2116.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 25.30it/s, est. speed input: 7633.54 toks/s, output: 86.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:57<00:00,  8.70it/s, est. speed input: 1338.41 toks/s, output: 2172.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.21it/s, est. speed input: 8062.23 toks/s, output: 84.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:56<00:00,  8.90it/s, est. speed input: 1379.11 toks/s, output: 2220.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:19<00:00, 26.25it/s, est. speed input: 8061.84 toks/s, output: 86.46 toks/s]\n",
      "  5%|▍         | 46/1000 [59:38<20:36:47, 77.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     prompt_list\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#interference\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m predicted_text_list\u001b[38;5;241m=\u001b[39m\u001b[43mllm_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#predict properties according to the generated reasonings.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m check_prompt_list,masked_reason_list\u001b[38;5;241m=\u001b[39mgen_masked_prediction_problem_prompts(predicted_text_list,problems,tokenizer)\n",
      "File \u001b[0;32m/data/hatakeyama/self-loop/0918repeat_prediction/src/llm/PredictionUtils.py:28\u001b[0m, in \u001b[0;36mllm_gen\u001b[0;34m(model, prompt_list, temperature, top_k, max_tokens)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm_gen\u001b[39m(model, prompt_list,\n\u001b[1;32m     25\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     26\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     27\u001b[0m             ):\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSamplingParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [i\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/utils.py:1036\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1033\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         )\n\u001b[0;32m-> 1036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/entrypoints/llm.py:348\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request)\u001b[0m\n\u001b[1;32m    339\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    342\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    343\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[1;32m    344\u001b[0m     lora_request\u001b[38;5;241m=\u001b[39mlora_request,\n\u001b[1;32m    345\u001b[0m     prompt_adapter_request\u001b[38;5;241m=\u001b[39mprompt_adapter_request,\n\u001b[1;32m    346\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request)\n\u001b[0;32m--> 348\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/entrypoints/llm.py:715\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    713\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 715\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/engine/llm_engine.py:1223\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1220\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1221\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:78\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_worker_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_worker_execution_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m         async_run_tensor_parallel_workers_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m driver_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver_execute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m driver_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:162\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._driver_execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_driver_execute_model\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: Optional[ExecuteModelRequest]\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[SamplerOutput]]:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run execute_model in the driver worker.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Passing None will cause the driver to stop the model execution\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    loop running in each of the remote workers.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/worker/worker_base.py:327\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    324\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 327\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/worker/model_runner_base.py:112\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    114\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/worker/model_runner.py:1546\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1543\u001b[0m     model_forward_end \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mEvent(enable_timing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1544\u001b[0m     model_forward_start\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m-> 1546\u001b[0m hidden_or_intermediate_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mMultiModalInputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_modal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseqlen_agnostic_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time):\n\u001b[1;32m   1558\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39mrecord()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:448\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    442\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m     intermediate_tensors: Optional[IntermediateTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    447\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 448\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:329\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    328\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 329\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    341\u001b[0m     })\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:251\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    250\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 251\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    259\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    260\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:181\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    179\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    180\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n\u001b[0;32m--> 181\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/attention/layer.py:98\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, attn_type)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     attn_type: AttentionType \u001b[38;5;241m=\u001b[39m AttentionType\u001b[38;5;241m.\u001b[39mDECODER,\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/attention/backends/flash_attn.py:725\u001b[0m, in \u001b[0;36mFlashAttentionImpl.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, k_scale, v_scale, attn_type)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefill_meta \u001b[38;5;241m:=\u001b[39m attn_metadata\u001b[38;5;241m.\u001b[39mprefill_metadata:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# Prompt run.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m prefill_meta\u001b[38;5;241m.\u001b[39mblock_tables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m prefill_meta\u001b[38;5;241m.\u001b[39mblock_tables\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;66;03m# normal attention\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;66;03m# When block_tables are not filled, it means q and k are the\u001b[39;00m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;66;03m# prompt, and they have the same length.\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m         prefill_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflash_attn_varlen_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefill_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_start_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefill_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_start_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefill_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_prefill_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefill_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_prefill_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_soft_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;66;03m# prefix-enabled attention\u001b[39;00m\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m prefill_meta\u001b[38;5;241m.\u001b[39mseq_lens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_library/custom_ops.py:236\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackend_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Checks the assumption that outputs cannot alias\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# inputs or other outputs.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     storages \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28mid\u001b[39m(tensor\u001b[38;5;241m.\u001b[39muntyped_storage())\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m iter_tensors(args, kwargs)\n\u001b[1;32m    234\u001b[0m     }\n\u001b[0;32m--> 236\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     tuple_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm/attention/backends/flash_attn.py:49\u001b[0m, in \u001b[0;36mflash_attn_varlen_func\u001b[0;34m(q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, softmax_scale, causal, window_size, softcap, alibi_slopes, block_table)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(window_size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     48\u001b[0m     real_window_size \u001b[38;5;241m=\u001b[39m (window_size[\u001b[38;5;241m0\u001b[39m], window_size[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flash_attn_varlen_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/vllm_flash_attn/flash_attn_interface.py:1154\u001b[0m, in \u001b[0;36mflash_attn_varlen_func\u001b[0;34m(q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs, block_table, out)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflash_attn_varlen_func\u001b[39m(\n\u001b[1;32m   1080\u001b[0m     q,\n\u001b[1;32m   1081\u001b[0m     k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1098\u001b[0m ):\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m    Supports multi-query and grouped-query attention (MQA/GQA) by passing in K, V with fewer heads\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlashAttnVarlenFunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attn_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py:573\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     args \u001b[38;5;241m=\u001b[39m bind_default_args(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mforward, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43m_functorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_dead_wrappers\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_functorch/utils.py:33\u001b[0m, in \u001b[0;36munwrap_dead_wrappers\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munwrap_dead_wrappers\u001b[39m(args):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# NB: doesn't use tree_map_only for performance reasons\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43munwrap_if_dead\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_functorch/utils.py:34\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munwrap_dead_wrappers\u001b[39m(args):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# NB: doesn't use tree_map_only for performance reasons\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m---> 34\u001b[0m         unwrap_if_dead(arg) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "n_records=500\n",
    "\n",
    "generated_records=[]\n",
    "for _ in tqdm(range(10**3)):\n",
    "    problems=[]\n",
    "    prompt_list=[\n",
    "    ]\n",
    "    for i in range(n_records):\n",
    "        record=random.choice(train_ds)\n",
    "        #prompt,actual_value=gen_problem(record,tokenizer)\n",
    "        prompt,actual_value=gen_reason(record,tokenizer)\n",
    "        problems.append(\n",
    "            {\n",
    "            \"record\":record,\n",
    "            \"prompt\":prompt,\n",
    "            \"actual_value\":actual_value   \n",
    "            }\n",
    "\n",
    "        )\n",
    "        prompt_list.append(prompt)\n",
    "\n",
    "    #interference\n",
    "    predicted_text_list=llm_gen(model,prompt_list)\n",
    "    \n",
    "    #predict properties according to the generated reasonings.\n",
    "    check_prompt_list,masked_reason_list=gen_masked_prediction_problem_prompts(predicted_text_list,problems,tokenizer)\n",
    "    predicted_value_list=llm_gen(model,check_prompt_list)\n",
    "\n",
    "\n",
    "    #parse result\n",
    "    #prediction_records=parse_prediction(problems,predicted_text_list)\n",
    "    prediction_records=parse_prediction_with_check(problems, masked_reason_list, predicted_value_list)\n",
    "\n",
    "    generated_records.extend(prediction_records)\n",
    "    for i in range(len(prediction_records)):\n",
    "        with open(f\"data/{current_time_no_symbols}_llm_gen.jsonl\",\"a\", encoding='utf-8') as f:\n",
    "            f.write(json.dumps(prediction_records[i],ensure_ascii=False)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
