{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "\n",
    "os.environ[\"LIBRARY_PATH\"]=\"/usr/local/cuda-12.2/lib64/stubs:$LIBRARY_PATH\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 15:53:29 config.py:904] Defaulting to use mp for distributed inference\n",
      "INFO 09-19 15:53:29 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-19 15:53:30 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-19 15:53:30 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:53:30 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:30 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m INFO 09-19 15:53:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m INFO 09-19 15:53:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m INFO 09-19 15:53:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m INFO 09-19 15:53:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:53:31 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:53:36 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:53:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hatakeyama/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-19 15:53:40 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ff1a102ac50>, local_subscribe_port=33455, remote_subscribe_port=None)\n",
      "INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:53:40 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:53:41 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bb06f6344347e4b22bf53d059fa8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 15:53:48 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m INFO 09-19 15:53:49 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m INFO 09-19 15:53:49 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m INFO 09-19 15:53:49 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:53:50 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:53:50 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m INFO 09-19 15:53:51 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:53:51 model_runner.py:1008] Loading model weights took 16.4634 GB\n",
      "INFO 09-19 15:53:58 distributed_gpu_executor.py:57] # GPU blocks: 81384, # CPU blocks: 6553\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:54:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m INFO 09-19 15:54:01 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-19 15:54:01 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-19 15:54:01 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-19 15:54:01 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "INFO 09-19 15:54:31 custom_all_reduce.py:223] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677203)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677198)\u001b[0;0m INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677199)\u001b[0;0m INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677197)\u001b[0;0m INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=677202)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677200)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=677201)\u001b[0;0m INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n",
      "INFO 09-19 15:54:31 model_runner.py:1430] Graph capturing finished in 31 secs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from src.llm.PredictionUtils import init_model_and_tokenizer, llm_gen\n",
    "\n",
    "\n",
    "model, tokenizer = init_model_and_tokenizer(model_id,\n",
    "                                            enable_lora=True,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24889"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds=load_dataset(\"kanhatakeyama/material-properties\",split=\"Bradley\")\n",
    "ds=ds.shuffle(seed=1)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CompName': '(2Z)-2-(1,3-Benzodioxol-5-ylmethylene)-6,7-dihydro-5H-[1,3]thiazolo[3,2-a]pyrimidin-3(2H)-one',\n",
       " 'SMILES': 'O=C1C(SC2=NCCCN12)=Cc4ccc3OCOc3c4',\n",
       " 'Property': 'Melting temperature',\n",
       " 'Value': 151.35,\n",
       " 'unit': '[oC]',\n",
       " 'Source': 'BradleyMeltingPointDataset'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ds.select(range(24000))\n",
    "test_ds = ds.select(range(24000, 24800))\n",
    "train_ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "prompt_list=[]\n",
    "problems=[]\n",
    "n_records=200\n",
    "target_ds=test_ds\n",
    "n_records=min(n_records,len(target_ds))\n",
    "for i in range(n_records):\n",
    "    record=random.choice(target_ds)\n",
    "    q=f\"Predict \"+record[\"Property\"]+\" \"+record[\"unit\"]+\" for \"+record[\"CompName\"]+\" (Compound X) with SMILES \"+str(record[\"SMILES\"])+\". The prediction consists of #Reason and #Prediction. The #Reason is the quantitative explanation of the prediction. The #Prediction is the predicted value and the unit of the prediction.\"\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "    prompt=tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "    prompt_list.append(prompt+ \"assistant\\n\\n#Reason\\n\")\n",
    "    problems.append(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 200/200 [01:03<00:00,  3.13it/s, est. speed input: 374.35 toks/s, output: 789.15 toks/s]\n",
      "  0%|          | 0/4 [01:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "parse_Q_R_A_prediction() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m cnt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m predicted_texts\u001b[38;5;241m=\u001b[39mllm_gen(model,prompt_list,\n\u001b[1;32m     13\u001b[0m                         enable_lora\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                         lora_path\u001b[38;5;241m=\u001b[39mlora_model,\n\u001b[1;32m     15\u001b[0m                         lora_id\u001b[38;5;241m=\u001b[39mcnt)\n\u001b[0;32m---> 16\u001b[0m good_records\u001b[38;5;241m=\u001b[39m\u001b[43mparse_Q_R_A_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproblems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m prediction_dict[lora_model]\u001b[38;5;241m=\u001b[39mgood_records\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_Q_R_A_prediction() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "from src.prop.utils import parse_Q_R_A_prediction\n",
    "lora_model_list=glob.glob(\"output0919_threshold_0.1_lora_kqvo_proj/checkpoint-*\")\n",
    "lora_model_list= sorted(lora_model_list, key=lambda x: int(x.split('-')[-1]))\n",
    "\n",
    "prediction_dict={}\n",
    "\n",
    "cnt=0\n",
    "for lora_model in tqdm(lora_model_list[:]):\n",
    "    cnt+=1\n",
    "    predicted_texts=llm_gen(model,prompt_list,\n",
    "                            enable_lora=True,\n",
    "                            lora_path=lora_model,\n",
    "                            lora_id=cnt)\n",
    "    good_records=parse_Q_R_A_prediction(predicted_texts,problems)\n",
    "    prediction_dict[lora_model]=good_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_texts=llm_gen(model,prompt_list,\n",
    "                        enable_lora=False,\n",
    "                        )\n",
    "good_records=parse_Q_R_A_prediction(predicted_texts)\n",
    "prediction_dict[\"original\"]=good_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json \n",
    "#with open(\"data/predict/0919prediction_dict_kqvo_proj.json\",\"w\") as f:\n",
    "#    json.dump(prediction_dict,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lora_name,good_records in prediction_dict.items():\n",
    "\n",
    "    df=pd.DataFrame(good_records)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.histplot(df[df[\"error_rate\"]<2], x=\"error_rate\", hue=\"Property\", multiple=\"stack\", \n",
    "                bins=20)\n",
    "\n",
    "\n",
    "    s_df=df[df[\"error_rate\"]<1]\n",
    "    median1=s_df[\"error_rate\"].median()\n",
    "    median2=df[\"error_rate\"].median()\n",
    "    plt.title(f\"lora model {lora_name} {median1} {median2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
